{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4NCKFnfidRgNF5MmPjLyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaherMo/Bdatatask10/blob/main/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w5Q65vKN90m",
        "outputId": "3d06c241-e4d2-4990-abfe-fe6bbddd63cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "_LSd4PS-OQIM",
        "outputId": "84173d78-7791-49f8-8d8f-38496aacdad9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb894734850>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://52da4620809d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***PRE-PROCESSING***"
      ],
      "metadata": {
        "id": "T8CWD6G4K8S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown library\n",
        "!pip install gdown\n",
        "\n",
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Define the file ID and output file name\n",
        "file_id = '1XPTMjsJrFFZTiVhSm5Q--gfkbm8jQGnO'\n",
        "output_file = 'your_dataset.csv'\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', output_file, quiet=False)\n",
        "\n",
        "# Read the dataset using PySpark\n",
        "df_spark = spark.read.csv(output_file, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df_spark.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzFFoTZqPTK2",
        "outputId": "cb239545-b5ce-42ed-80f0-77deb39fb963"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XPTMjsJrFFZTiVhSm5Q--gfkbm8jQGnO\n",
            "To: /content/your_dataset.csv\n",
            "100%|██████████| 3.81M/3.81M [00:00<00:00, 27.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+------------+-------------+---------------+-----+-----------+-------------------+--------+\n",
            "|gender| age|hypertension|heart_disease|smoking_history|  bmi|HbA1c_level|blood_glucose_level|diabetes|\n",
            "+------+----+------------+-------------+---------------+-----+-----------+-------------------+--------+\n",
            "|Female|80.0|           0|            1|          never|25.19|        6.6|                140|       0|\n",
            "|Female|54.0|           0|            0|        No Info|27.32|        6.6|                 80|       0|\n",
            "|  Male|28.0|           0|            0|          never|27.32|        5.7|                158|       0|\n",
            "|Female|36.0|           0|            0|        current|23.45|        5.0|                155|       0|\n",
            "|  Male|76.0|           1|            1|        current|20.14|        4.8|                155|       0|\n",
            "|Female|20.0|           0|            0|          never|27.32|        6.6|                 85|       0|\n",
            "|Female|44.0|           0|            0|          never|19.31|        6.5|                200|       1|\n",
            "|Female|79.0|           0|            0|        No Info|23.86|        5.7|                 85|       0|\n",
            "|  Male|42.0|           0|            0|          never|33.64|        4.8|                145|       0|\n",
            "|Female|32.0|           0|            0|          never|27.32|        5.0|                100|       0|\n",
            "|Female|53.0|           0|            0|          never|27.32|        6.1|                 85|       0|\n",
            "|Female|54.0|           0|            0|         former| 54.7|        6.0|                100|       0|\n",
            "|Female|78.0|           0|            0|         former|36.05|        5.0|                130|       0|\n",
            "|Female|67.0|           0|            0|          never|25.69|        5.8|                200|       0|\n",
            "|Female|76.0|           0|            0|        No Info|27.32|        5.0|                160|       0|\n",
            "|  Male|78.0|           0|            0|        No Info|27.32|        6.6|                126|       0|\n",
            "|  Male|15.0|           0|            0|          never|30.36|        6.1|                200|       0|\n",
            "|Female|42.0|           0|            0|          never|24.48|        5.7|                158|       0|\n",
            "|Female|42.0|           0|            0|        No Info|27.32|        5.7|                 80|       0|\n",
            "|  Male|37.0|           0|            0|           ever|25.72|        3.5|                159|       0|\n",
            "+------+----+------------+-------------+---------------+-----+-----------+-------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for missing values"
      ],
      "metadata": {
        "id": "JoSifCEtK4O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Check for missing values\n",
        "missing_values_count = df_spark.select([col(c).isNull().cast(\"int\").alias(c) for c in df_spark.columns]).groupBy().sum().collect()[0]\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "for col_name, missing_count in zip(df_spark.columns, missing_values_count):\n",
        "    print(f\"Number of missing values in {col_name}: {missing_count}\")\n",
        "\n",
        "# Total number of missing values\n",
        "total_missing_values = sum(missing_values_count)\n",
        "print(f\"Total number of missing values = {total_missing_values}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYr0nOBXPZ8T",
        "outputId": "32cc2af0-4955-4d2a-d38a-aac32a6f94de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in gender: 0\n",
            "Number of missing values in age: 0\n",
            "Number of missing values in hypertension: 0\n",
            "Number of missing values in heart_disease: 0\n",
            "Number of missing values in smoking_history: 0\n",
            "Number of missing values in bmi: 0\n",
            "Number of missing values in HbA1c_level: 0\n",
            "Number of missing values in blood_glucose_level: 0\n",
            "Number of missing values in diabetes: 0\n",
            "Total number of missing values = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics\n",
        "df_spark.describe().show()\n",
        "\n",
        "# Display schema information\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Display the number of rows and columns\n",
        "num_rows = df_spark.count()\n",
        "num_cols = len(df_spark.columns)\n",
        "\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_cols}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pmIiyIlQKkw",
        "outputId": "9149108f-497a-4ecf-fa43-c742ddc386bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------------+------------------+------------------+---------------+-----------------+------------------+-------------------+-------------------+\n",
            "|summary|gender|              age|      hypertension|     heart_disease|smoking_history|              bmi|       HbA1c_level|blood_glucose_level|           diabetes|\n",
            "+-------+------+-----------------+------------------+------------------+---------------+-----------------+------------------+-------------------+-------------------+\n",
            "|  count|100000|           100000|            100000|            100000|         100000|           100000|            100000|             100000|             100000|\n",
            "|   mean|  null|41.88585600000013|           0.07485|           0.03942|           null|27.32076709999422|5.5275069999983275|          138.05806|              0.085|\n",
            "| stddev|  null|22.51683987161704|0.2631504702289171|0.1945930169980986|           null|6.636783416648357|1.0706720918835468|  40.70813604870383|0.27888308976661896|\n",
            "|    min|Female|             0.08|                 0|                 0|        No Info|            10.01|               3.5|                 80|                  0|\n",
            "|    max| Other|             80.0|                 1|                 1|    not current|            95.69|               9.0|                300|                  1|\n",
            "+-------+------+-----------------+------------------+------------------+---------------+-----------------+------------------+-------------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- gender: string (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- hypertension: integer (nullable = true)\n",
            " |-- heart_disease: integer (nullable = true)\n",
            " |-- smoking_history: string (nullable = true)\n",
            " |-- bmi: double (nullable = true)\n",
            " |-- HbA1c_level: double (nullable = true)\n",
            " |-- blood_glucose_level: integer (nullable = true)\n",
            " |-- diabetes: integer (nullable = true)\n",
            "\n",
            "Number of rows: 100000\n",
            "Number of columns: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with Duplicated Data"
      ],
      "metadata": {
        "id": "Yzc3a7hvKz_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "num_duplicates = df_spark.count() - df_spark.dropDuplicates().count()\n",
        "\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9MiCul-QWKm",
        "outputId": "a36bedd1-a571-452c-f1a7-dc4562398fe2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 3854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows\n",
        "df_spark = df_spark.dropDuplicates()\n"
      ],
      "metadata": {
        "id": "QBhzKfmUQjrx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Categorical and Continous Variables"
      ],
      "metadata": {
        "id": "NYjpwbCJKqwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'df2' with the actual name of your PySpark DataFrame\n",
        "cont_cols = [col_name for col_name, data_type in df_spark.dtypes if data_type in ['int', 'bigint', 'float', 'double']]\n",
        "cat_cols = list(set(df_spark.columns) - set(cont_cols))\n",
        "\n",
        "# Print the continuous variables\n",
        "print(\"The continuous variables are:\", cont_cols)\n",
        "\n",
        "# Print the categorical variables\n",
        "print(\"The categorical variables are:\", cat_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpc9SJ7qQqYV",
        "outputId": "cf0f0a78-d298-4494-87c6-9de54ccb4648"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The continuous variables are: ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']\n",
            "The categorical variables are: ['smoking_history', 'gender']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Categorical data into Numeric data"
      ],
      "metadata": {
        "id": "tIx99c1EKi8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define the columns to be converted\n",
        "categorical_columns = ['gender', 'smoking_history']\n",
        "\n",
        "# Create a StringIndexer for each categorical column\n",
        "indexers = [StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\") for col_name in categorical_columns]\n",
        "\n",
        "# Create a pipeline to execute the indexers\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "\n",
        "# Fit and transform the DataFrame using the pipeline\n",
        "df_spark_indexed = pipeline.fit(df_spark).transform(df_spark)\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "df_spark_indexed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcPgNLedSEpy",
        "outputId": "dfec3792-10ac-4ca8-a541-94ad53ee0394"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+------------+-------------+---------------+-----+-----------+-------------------+--------+------------+---------------------+\n",
            "|gender| age|hypertension|heart_disease|smoking_history|  bmi|HbA1c_level|blood_glucose_level|diabetes|gender_index|smoking_history_index|\n",
            "+------+----+------------+-------------+---------------+-----+-----------+-------------------+--------+------------+---------------------+\n",
            "|Female|21.0|           0|            0|          never|27.32|        5.8|                126|       0|         0.0|                  0.0|\n",
            "|  Male|26.0|           0|            0|          never|27.32|        6.6|                100|       0|         1.0|                  0.0|\n",
            "|Female|49.0|           0|            0|          never| 21.7|        5.8|                158|       0|         0.0|                  0.0|\n",
            "|Female|24.0|           0|            0|         former|20.47|        4.8|                100|       0|         0.0|                  2.0|\n",
            "|Female|53.0|           0|            0|          never| 31.4|        5.7|                 85|       0|         0.0|                  0.0|\n",
            "|Female|74.0|           0|            0|         former| 40.5|        3.5|                160|       0|         0.0|                  2.0|\n",
            "|  Male|76.0|           0|            0|         former|27.76|        6.5|                158|       0|         1.0|                  2.0|\n",
            "|  Male| 5.0|           0|            0|        No Info| 17.9|        6.6|                126|       0|         1.0|                  1.0|\n",
            "|Female|55.0|           0|            0|          never|42.64|        6.0|                155|       1|         0.0|                  0.0|\n",
            "|  Male|68.0|           1|            0|          never|42.15|        6.2|                145|       1|         1.0|                  0.0|\n",
            "|Female|26.0|           0|            0|          never|39.93|        4.0|                160|       0|         0.0|                  0.0|\n",
            "|Female|32.0|           0|            0|          never|27.32|        4.5|                200|       0|         0.0|                  0.0|\n",
            "|Female|62.0|           0|            0|          never|25.71|        5.0|                145|       0|         0.0|                  0.0|\n",
            "|  Male|0.48|           0|            0|        No Info|15.19|        4.8|                140|       0|         1.0|                  1.0|\n",
            "|Female|17.0|           0|            0|    not current|18.71|        5.0|                200|       0|         0.0|                  4.0|\n",
            "|  Male|15.0|           0|            0|        No Info| 23.2|        3.5|                155|       0|         1.0|                  1.0|\n",
            "|Female|51.0|           0|            0|         former|27.32|        3.5|                160|       0|         0.0|                  2.0|\n",
            "|Female|73.0|           1|            0|          never| 22.8|        9.0|                126|       1|         0.0|                  0.0|\n",
            "|  Male|49.0|           0|            0|          never| 32.5|        5.8|                145|       0|         1.0|                  0.0|\n",
            "|Female|60.0|           0|            0|          never|45.98|        6.5|                159|       1|         0.0|                  0.0|\n",
            "+------+----+------------+-------------+---------------+-----+-----------+-------------------+--------+------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop original columns\n",
        "df_spark_indexed = df_spark_indexed.drop('gender', 'smoking_history')\n",
        "\n",
        "# Rename indexed columns\n",
        "df_spark_indexed = df_spark_indexed.withColumnRenamed('gender_index', 'gender').withColumnRenamed('smoking_history_index', 'smoking_history')\n",
        "\n",
        "# Show the updated DataFrame\n",
        "df_spark_indexed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGXklmtzTTX2",
        "outputId": "aa124245-63a5-4fce-fa20-6586a5744d58"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "| age|hypertension|heart_disease|  bmi|HbA1c_level|blood_glucose_level|diabetes|gender|smoking_history|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "|21.0|           0|            0|27.32|        5.8|                126|       0|   0.0|            0.0|\n",
            "|26.0|           0|            0|27.32|        6.6|                100|       0|   1.0|            0.0|\n",
            "|49.0|           0|            0| 21.7|        5.8|                158|       0|   0.0|            0.0|\n",
            "|24.0|           0|            0|20.47|        4.8|                100|       0|   0.0|            2.0|\n",
            "|53.0|           0|            0| 31.4|        5.7|                 85|       0|   0.0|            0.0|\n",
            "|74.0|           0|            0| 40.5|        3.5|                160|       0|   0.0|            2.0|\n",
            "|76.0|           0|            0|27.76|        6.5|                158|       0|   1.0|            2.0|\n",
            "| 5.0|           0|            0| 17.9|        6.6|                126|       0|   1.0|            1.0|\n",
            "|55.0|           0|            0|42.64|        6.0|                155|       1|   0.0|            0.0|\n",
            "|68.0|           1|            0|42.15|        6.2|                145|       1|   1.0|            0.0|\n",
            "|26.0|           0|            0|39.93|        4.0|                160|       0|   0.0|            0.0|\n",
            "|32.0|           0|            0|27.32|        4.5|                200|       0|   0.0|            0.0|\n",
            "|62.0|           0|            0|25.71|        5.0|                145|       0|   0.0|            0.0|\n",
            "|0.48|           0|            0|15.19|        4.8|                140|       0|   1.0|            1.0|\n",
            "|17.0|           0|            0|18.71|        5.0|                200|       0|   0.0|            4.0|\n",
            "|15.0|           0|            0| 23.2|        3.5|                155|       0|   1.0|            1.0|\n",
            "|51.0|           0|            0|27.32|        3.5|                160|       0|   0.0|            2.0|\n",
            "|73.0|           1|            0| 22.8|        9.0|                126|       1|   0.0|            0.0|\n",
            "|49.0|           0|            0| 32.5|        5.8|                145|       0|   1.0|            0.0|\n",
            "|60.0|           0|            0|45.98|        6.5|                159|       1|   0.0|            0.0|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Info"
      ],
      "metadata": {
        "id": "93CV8pPjKeGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "# Define the numeric columns\n",
        "numeric_columns = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']\n",
        "\n",
        "# Display descriptive statistics for each feature\n",
        "for col_name in numeric_columns:\n",
        "    df_spark_indexed.select(col(col_name)).describe().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnaZT410TWEI",
        "outputId": "5678c1a6-01cf-4e18-9e44-745266ecede5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|summary|               age|\n",
            "+-------+------------------+\n",
            "|  count|             96146|\n",
            "|   mean| 41.79432571297818|\n",
            "| stddev|22.462947577419353|\n",
            "|    min|              0.08|\n",
            "|    max|              80.0|\n",
            "+-------+------------------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|summary|       hypertension|\n",
            "+-------+-------------------+\n",
            "|  count|              96146|\n",
            "|   mean|0.07760073221974913|\n",
            "| stddev|0.26754364703227407|\n",
            "|    min|                  0|\n",
            "|    max|                  1|\n",
            "+-------+-------------------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|summary|      heart_disease|\n",
            "+-------+-------------------+\n",
            "|  count|              96146|\n",
            "|   mean|0.04080252948640609|\n",
            "| stddev|0.19783349095456876|\n",
            "|    min|                  0|\n",
            "|    max|                  1|\n",
            "+-------+-------------------+\n",
            "\n",
            "+-------+------------------+\n",
            "|summary|               bmi|\n",
            "+-------+------------------+\n",
            "|  count|             96146|\n",
            "|   mean|27.321461111226636|\n",
            "| stddev| 6.767715560480339|\n",
            "|    min|             10.01|\n",
            "|    max|             95.69|\n",
            "+-------+------------------+\n",
            "\n",
            "+-------+------------------+\n",
            "|summary|       HbA1c_level|\n",
            "+-------+------------------+\n",
            "|  count|             96146|\n",
            "|   mean| 5.532608740873252|\n",
            "| stddev|1.0732320256443277|\n",
            "|    min|               3.5|\n",
            "|    max|               9.0|\n",
            "+-------+------------------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|summary|blood_glucose_level|\n",
            "+-------+-------------------+\n",
            "|  count|              96146|\n",
            "|   mean| 138.21823060761758|\n",
            "| stddev| 40.909771362746795|\n",
            "|    min|                 80|\n",
            "|    max|                300|\n",
            "+-------+-------------------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|summary|           diabetes|\n",
            "+-------+-------------------+\n",
            "|  count|              96146|\n",
            "|   mean|0.08821999875189815|\n",
            "| stddev|0.28361605595642964|\n",
            "|    min|                  0|\n",
            "|    max|                  1|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Undersampling"
      ],
      "metadata": {
        "id": "gvffXfB7KYE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Add a unique index column to df_spark_indexed\n",
        "df_spark_indexed_with_index = df_spark_indexed.withColumn(\"index\", F.monotonically_increasing_id())\n",
        "\n",
        "# Before undersampling\n",
        "original_class_distribution = df_spark_indexed_with_index.groupBy('diabetes').count().show()\n",
        "print(\"Original Class Distribution:\")\n",
        "original_class_distribution\n",
        "\n",
        "# Define a Window specification\n",
        "window_spec = Window.partitionBy('diabetes').orderBy('index')\n",
        "\n",
        "# Undersample each group (diabetes=0 and diabetes=1) to have exactly 8482 rows\n",
        "undersampled_df = df_spark_indexed_with_index.withColumn(\n",
        "    'row_num',\n",
        "    F.row_number().over(window_spec)\n",
        ").filter('row_num <= 8482').drop('index', 'row_num')\n",
        "\n",
        "# After undersampling\n",
        "undersampled_class_distribution = undersampled_df.groupBy('diabetes').count().show()\n",
        "print(\"Undersampled Class Distribution:\")\n",
        "undersampled_class_distribution\n",
        "\n",
        "# Check the number of rows in the undersampled DataFrame\n",
        "num_rows_undersampled = undersampled_df.count()\n",
        "print(\"Number of Rows in Undersampled DataFrame: {}\".format(num_rows_undersampled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUFOLy0yYmWo",
        "outputId": "24088103-73d5-45b0-f259-1c5d76ab86c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|diabetes|count|\n",
            "+--------+-----+\n",
            "|       1| 8482|\n",
            "|       0|87664|\n",
            "+--------+-----+\n",
            "\n",
            "Original Class Distribution:\n",
            "+--------+-----+\n",
            "|diabetes|count|\n",
            "+--------+-----+\n",
            "|       1| 8482|\n",
            "|       0| 8482|\n",
            "+--------+-----+\n",
            "\n",
            "Undersampled Class Distribution:\n",
            "Number of Rows in Undersampled DataFrame: 16964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "undersampled_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--D6AfO6cQ-D",
        "outputId": "7054cead-68ed-4cb6-983b-b329e54fdd73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "| age|hypertension|heart_disease|  bmi|HbA1c_level|blood_glucose_level|diabetes|gender|smoking_history|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "|55.0|           0|            0|42.64|        6.0|                155|       1|   0.0|            0.0|\n",
            "|68.0|           1|            0|42.15|        6.2|                145|       1|   1.0|            0.0|\n",
            "|73.0|           1|            0| 22.8|        9.0|                126|       1|   0.0|            0.0|\n",
            "|60.0|           0|            0|45.98|        6.5|                159|       1|   0.0|            0.0|\n",
            "|80.0|           0|            1| 27.8|        8.8|                145|       1|   1.0|            2.0|\n",
            "|74.0|           0|            1|28.83|        6.0|                280|       1|   0.0|            0.0|\n",
            "|62.0|           0|            0|27.32|        5.7|                260|       1|   0.0|            0.0|\n",
            "|14.0|           0|            0|19.97|        8.2|                260|       1|   0.0|            0.0|\n",
            "|52.0|           0|            0| 42.1|        6.1|                145|       1|   1.0|            0.0|\n",
            "|66.0|           1|            0|26.26|        5.8|                145|       1|   0.0|            0.0|\n",
            "|80.0|           0|            0|27.32|        7.0|                130|       1|   0.0|            0.0|\n",
            "|80.0|           0|            0|24.93|        6.8|                126|       1|   1.0|            1.0|\n",
            "|62.0|           0|            0|34.49|        5.7|                160|       1|   0.0|            2.0|\n",
            "|61.0|           0|            0|23.96|        6.2|                200|       1|   1.0|            3.0|\n",
            "|74.0|           1|            1|31.71|        5.7|                126|       1|   1.0|            2.0|\n",
            "|80.0|           0|            0|27.32|        6.0|                126|       1|   0.0|            1.0|\n",
            "|80.0|           0|            0|27.32|        6.5|                300|       1|   1.0|            2.0|\n",
            "|61.0|           0|            0|38.04|        5.7|                280|       1|   0.0|            0.0|\n",
            "|57.0|           0|            0|27.32|        5.7|                159|       1|   1.0|            2.0|\n",
            "|51.0|           1|            0|31.75|        6.1|                280|       1|   0.0|            0.0|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Outliers"
      ],
      "metadata": {
        "id": "crPzH-wpKTHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "# Define the feature columns (excluding the target column)\n",
        "feature_columns = [col_name for col_name in df_spark_cleaned.columns if col_name != 'diabetes']\n",
        "\n",
        "# Standardize the features\n",
        "for col_name in feature_columns:\n",
        "    mean_col = F.mean(col(col_name)).over(Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n",
        "    std_col = F.stddev(col(col_name)).over(Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n",
        "    col_name_z = f'{col_name}_z'\n",
        "    df_spark_cleaned = df_spark_cleaned.withColumn(col_name_z, (col(col_name) - mean_col) / std_col)\n",
        "\n",
        "# Define a threshold to identify outliers (e.g., 3 standard deviations)\n",
        "threshold = 3\n",
        "\n",
        "# Create a boolean mask to identify rows with outliers\n",
        "outlier_mask_expr = ' OR '.join([f'abs({col_name}_z) > {threshold}' for col_name in feature_columns])\n",
        "df_spark_cleaned = df_spark_cleaned.filter(~F.expr(outlier_mask_expr))\n",
        "\n",
        "# Drop the Z-score columns\n",
        "df_spark_cleaned = df_spark_cleaned.drop(*[f'{col_name}_z' for col_name in feature_columns])\n",
        "\n",
        "# Show the cleaned DataFrame\n",
        "df_spark_cleaned.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKMuDB3TTe5i",
        "outputId": "33aad378-b4d7-47a1-e1ac-d29df0f102d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "| age|hypertension|heart_disease|  bmi|HbA1c_level|blood_glucose_level|diabetes|gender|smoking_history|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "|55.0|           0|            0|42.64|        6.0|                155|       1|   0.0|            0.0|\n",
            "|68.0|           1|            0|42.15|        6.2|                145|       1|   1.0|            0.0|\n",
            "|73.0|           1|            0| 22.8|        9.0|                126|       1|   0.0|            0.0|\n",
            "|60.0|           0|            0|45.98|        6.5|                159|       1|   0.0|            0.0|\n",
            "|62.0|           0|            0|27.32|        5.7|                260|       1|   0.0|            0.0|\n",
            "|14.0|           0|            0|19.97|        8.2|                260|       1|   0.0|            0.0|\n",
            "|52.0|           0|            0| 42.1|        6.1|                145|       1|   1.0|            0.0|\n",
            "|66.0|           1|            0|26.26|        5.8|                145|       1|   0.0|            0.0|\n",
            "|80.0|           0|            0|27.32|        7.0|                130|       1|   0.0|            0.0|\n",
            "|80.0|           0|            0|24.93|        6.8|                126|       1|   1.0|            1.0|\n",
            "|62.0|           0|            0|34.49|        5.7|                160|       1|   0.0|            2.0|\n",
            "|61.0|           0|            0|23.96|        6.2|                200|       1|   1.0|            3.0|\n",
            "|80.0|           0|            0|27.32|        6.0|                126|       1|   0.0|            1.0|\n",
            "|80.0|           0|            0|27.32|        6.5|                300|       1|   1.0|            2.0|\n",
            "|61.0|           0|            0|38.04|        5.7|                280|       1|   0.0|            0.0|\n",
            "|57.0|           0|            0|27.32|        5.7|                159|       1|   1.0|            2.0|\n",
            "|51.0|           1|            0|31.75|        6.1|                280|       1|   0.0|            0.0|\n",
            "|46.0|           0|            0|40.39|        5.8|                280|       1|   0.0|            0.0|\n",
            "|73.0|           0|            0|33.03|        7.0|                140|       1|   1.0|            1.0|\n",
            "|79.0|           0|            0|27.32|        7.5|                126|       1|   1.0|            2.0|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_spark_cleaned\n",
        "df.show()\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQDq49xqfjVB",
        "outputId": "524d6a94-0dbb-4e45-b97e-e446a939f7c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "| age|hypertension|heart_disease|  bmi|HbA1c_level|blood_glucose_level|diabetes|gender|smoking_history|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "|55.0|           0|            0|42.64|        6.0|                155|       1|   0.0|            0.0|\n",
            "|68.0|           1|            0|42.15|        6.2|                145|       1|   1.0|            0.0|\n",
            "|73.0|           1|            0| 22.8|        9.0|                126|       1|   0.0|            0.0|\n",
            "|60.0|           0|            0|45.98|        6.5|                159|       1|   0.0|            0.0|\n",
            "|62.0|           0|            0|27.32|        5.7|                260|       1|   0.0|            0.0|\n",
            "|14.0|           0|            0|19.97|        8.2|                260|       1|   0.0|            0.0|\n",
            "|52.0|           0|            0| 42.1|        6.1|                145|       1|   1.0|            0.0|\n",
            "|66.0|           1|            0|26.26|        5.8|                145|       1|   0.0|            0.0|\n",
            "|80.0|           0|            0|27.32|        7.0|                130|       1|   0.0|            0.0|\n",
            "|80.0|           0|            0|24.93|        6.8|                126|       1|   1.0|            1.0|\n",
            "|62.0|           0|            0|34.49|        5.7|                160|       1|   0.0|            2.0|\n",
            "|61.0|           0|            0|23.96|        6.2|                200|       1|   1.0|            3.0|\n",
            "|80.0|           0|            0|27.32|        6.0|                126|       1|   0.0|            1.0|\n",
            "|80.0|           0|            0|27.32|        6.5|                300|       1|   1.0|            2.0|\n",
            "|61.0|           0|            0|38.04|        5.7|                280|       1|   0.0|            0.0|\n",
            "|57.0|           0|            0|27.32|        5.7|                159|       1|   1.0|            2.0|\n",
            "|51.0|           1|            0|31.75|        6.1|                280|       1|   0.0|            0.0|\n",
            "|46.0|           0|            0|40.39|        5.8|                280|       1|   0.0|            0.0|\n",
            "|73.0|           0|            0|33.03|        7.0|                140|       1|   1.0|            1.0|\n",
            "|79.0|           0|            0|27.32|        7.5|                126|       1|   1.0|            2.0|\n",
            "+----+------------+-------------+-----+-----------+-------------------+--------+------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15234"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting Data to Training and Test (80% and 20%)"
      ],
      "metadata": {
        "id": "ucyTEevhKJDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "feature_columns = [col_name for col_name in df_spark_cleaned.columns if col_name != 'diabetes']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "df_spark_assembled = vector_assembler.transform(df_spark_cleaned)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True, withStd=True)\n",
        "scaler_model = scaler.fit(df_spark_assembled)\n",
        "df_spark_scaled = scaler_model.transform(df_spark_assembled)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# Use randomSplit method\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1 - train_ratio\n",
        "seed = 42\n",
        "\n",
        "df_spark_train, df_spark_test = df_spark_scaled.randomSplit([train_ratio, test_ratio], seed=seed)\n",
        "\n",
        "# Show the dimensions of the resulting DataFrames\n",
        "print(\"Training set size:\", df_spark_train.count())\n",
        "print(\"Testing set size:\", df_spark_test.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNgms7n8zTpD",
        "outputId": "c4b76c17-b645-4e39-c35d-8be7b8067aef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 12243\n",
            "Testing set size: 2991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***MODELS*** **bold text**"
      ],
      "metadata": {
        "id": "rnuzH5Z-LIlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Model"
      ],
      "metadata": {
        "id": "1ZOgzV8VKGFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Assemble features into a single vector column with a different name\n",
        "feature_columns = [col_name for col_name in df_spark_train.columns if col_name != 'diabetes']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='assembled_features')\n",
        "df_spark_train_assembled = vector_assembler.transform(df_spark_train)\n",
        "df_spark_test_assembled = vector_assembler.transform(df_spark_test)\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "lr = LinearRegression(featuresCol='assembled_features', labelCol='diabetes')\n",
        "\n",
        "# Fit the model\n",
        "model = lr.fit(df_spark_train_assembled)\n",
        "\n",
        "# Make predictions on the training set\n",
        "train_predictions = model.transform(df_spark_train_assembled)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "test_predictions = model.transform(df_spark_test_assembled)\n",
        "\n",
        "# Evaluate R-squared on the training set\n",
        "evaluator_train = RegressionEvaluator(labelCol=\"diabetes\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2_train = evaluator_train.evaluate(train_predictions)\n",
        "print(\"R-squared on training set:\", r2_train)\n",
        "\n",
        "# Evaluate R-squared on the testing set\n",
        "evaluator_test = RegressionEvaluator(labelCol=\"diabetes\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2_test = evaluator_test.evaluate(test_predictions)\n",
        "print(\"R-squared on testing set:\", r2_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfvIVC_hznbe",
        "outputId": "fe34393d-e4be-46d0-8cc2-1ff01e1b17b0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared on training set: 0.5875790008080288\n",
            "R-squared on testing set: 0.5810031612628319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees Model"
      ],
      "metadata": {
        "id": "g_tFxCbuJ_r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create StringIndexer for categorical columns\n",
        "gender_indexer = StringIndexer(inputCol='gender', outputCol='gender_index')\n",
        "smoking_history_indexer = StringIndexer(inputCol='smoking_history', outputCol='smoking_history_index')\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "feature_columns = [col_name for col_name in df_spark_cleaned.columns if col_name not in ['diabetes', 'gender', 'smoking_history']]\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns + ['gender_index', 'smoking_history_index'], outputCol='features')\n",
        "\n",
        "# Initialize DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(featuresCol='features', labelCol='diabetes', seed=42)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[gender_indexer, smoking_history_indexer, vector_assembler, dt])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1 - train_ratio\n",
        "seed = 42\n",
        "\n",
        "df_spark_train, df_spark_test = df_spark_cleaned.randomSplit([train_ratio, test_ratio], seed=seed)\n",
        "\n",
        "# Fit the model\n",
        "model = pipeline.fit(df_spark_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "df_spark_predictions = model.transform(df_spark_test)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='diabetes', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(df_spark_predictions)\n",
        "df_spark_train_predictions = model.transform(df_spark_train)\n",
        "# Evaluate the model on training set\n",
        "evaluator_train = MulticlassClassificationEvaluator(labelCol='diabetes', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy_train = evaluator_train.evaluate(df_spark_train_predictions)\n",
        "\n",
        "print(\"Decision Tree Training Accuracy:\", accuracy_train)\n",
        "print(\"Decision Tree Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9FjZAam6WTn",
        "outputId": "13ae1f3a-aaf5-47d0-d9ea-8f7e819c94fd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Training Accuracy: 0.8909580985052683\n",
            "Decision Tree Test Accuracy: 0.8933467067870278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees Precision, Recall, F1 Scores and Confusion Matrix"
      ],
      "metadata": {
        "id": "6t55x9pFJ3Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "# Make predictions on the testing set\n",
        "df_spark_test_predictions = model.transform(df_spark_test)\n",
        "\n",
        "# Evaluate precision\n",
        "precision_evaluator = MulticlassClassificationEvaluator(labelCol='diabetes', predictionCol='prediction', metricName='weightedPrecision')\n",
        "precision = precision_evaluator.evaluate(df_spark_test_predictions)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Evaluate recall\n",
        "recall_evaluator = MulticlassClassificationEvaluator(labelCol='diabetes', predictionCol='prediction', metricName='weightedRecall')\n",
        "recall = recall_evaluator.evaluate(df_spark_test_predictions)\n",
        "\n",
        "print(\"Recall:\", recall)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxS5a0RD_ZAP",
        "outputId": "8385e31f-4fba-4f8c-e8b9-3111151bd9be"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8972908310644124\n",
            "Recall: 0.8933467067870278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Make predictions on the testing set\n",
        "df_spark_test_predictions = model.transform(df_spark_test)\n",
        "\n",
        "# Evaluate F1 score\n",
        "f1_evaluator = MulticlassClassificationEvaluator(labelCol='diabetes', predictionCol='prediction', metricName='f1')\n",
        "f1_score = f1_evaluator.evaluate(df_spark_test_predictions)\n",
        "\n",
        "print(\"F1 Score:\", f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBrpy7A-_yE3",
        "outputId": "e01db8b0-1416-4db0-de5c-b336722ba5d0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8933864804371636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Convert the PySpark DataFrame to an RDD of (prediction, label) tuples\n",
        "prediction_and_label = df_spark_test_predictions.select('prediction', 'diabetes').rdd.map(lambda row: (float(row['prediction']), float(row['diabetes'])))\n",
        "\n",
        "# Instantiate the MulticlassMetrics class\n",
        "metrics = MulticlassMetrics(prediction_and_label)\n",
        "\n",
        "# Get the confusion matrix\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUztlJZ7_1Vs",
        "outputId": "d78d5465-8c42-435f-8d58-b21be9ce1d7b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[1348.  229.]\n",
            " [  90. 1324.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model Accuracy"
      ],
      "metadata": {
        "id": "CnEwrv2dJy57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Check if 'new_features' column already exists\n",
        "if 'new_features' in df_spark_cleaned.columns:\n",
        "    # If it exists, drop the column\n",
        "    df_spark_cleaned = df_spark_cleaned.drop('new_features')\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "feature_columns = [col_name for col_name in df_spark_cleaned.columns if col_name != 'diabetes']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='new_features')\n",
        "df_spark_assembled = vector_assembler.transform(df_spark_cleaned)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# Use randomSplit method\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1 - train_ratio\n",
        "seed = 42\n",
        "df_spark_train, df_spark_test = df_spark_assembled.randomSplit([train_ratio, test_ratio], seed=seed)\n",
        "\n",
        "# Create a RandomForestClassifier without using VectorAssembler\n",
        "rf = RandomForestClassifier(featuresCol='new_features', labelCol='diabetes', seed=42)\n",
        "\n",
        "# Create a pipeline without VectorAssembler\n",
        "pipeline_rf = Pipeline(stages=[rf])\n",
        "\n",
        "# Train the model\n",
        "model_rf = pipeline_rf.fit(df_spark_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "df_spark_test_predictions_rf = model_rf.transform(df_spark_test)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='diabetes', metricName='accuracy')\n",
        "accuracy_rf = evaluator.evaluate(df_spark_test_predictions_rf)\n",
        "\n",
        "print(\"Random Forest Test Accuracy:\", accuracy_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jODKjSWeBGZ-",
        "outputId": "c70dcdd6-652b-4333-c80c-a179c9157847"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Test Accuracy: 0.8936810431293881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Precission, Recall, F1 Scores and Confusion Matrix"
      ],
      "metadata": {
        "id": "g7kKx1HhJrSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Make predictions on the testing set\n",
        "df_spark_test_predictions_rf = model_rf.transform(df_spark_test)\n",
        "\n",
        "# Evaluate the model using MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='diabetes', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy_rf = evaluator.evaluate(df_spark_test_predictions_rf)\n",
        "\n",
        "# Calculate precision, recall, and F1-score using MulticlassMetrics\n",
        "predictionAndLabels = df_spark_test_predictions_rf.select('prediction', 'diabetes').rdd.map(lambda row: (float(row['prediction']), float(row['diabetes'])))\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "\n",
        "# Precision\n",
        "precision_rf = metrics.precision(label=1.0)\n",
        "print(\"Precision:\", precision_rf)\n",
        "\n",
        "# Recall\n",
        "recall_rf = metrics.recall(label=1.0)\n",
        "print(\"Recall:\", recall_rf)\n",
        "\n",
        "# F1 Score\n",
        "f1_rf = metrics.fMeasure(label=1.0)\n",
        "print(\"F1 Score:\", f1_rf)\n",
        "\n",
        "# Confusion Matrix\n",
        "confusion_matrix_rf = metrics.confusionMatrix().toArray()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16OG4ph6BGfZ",
        "outputId": "3923c2c4-2c96-4e52-b4ab-0b5a03e77f52"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9376996805111821\n",
            "Recall: 0.8302687411598303\n",
            "F1 Score: 0.8807201800450113\n",
            "Confusion Matrix:\n",
            "[[1499.   78.]\n",
            " [ 240. 1174.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy for SVM Model"
      ],
      "metadata": {
        "id": "5ZdOj1_LJoX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "feature_columns = [col_name for col_name in df_spark_train.columns if col_name != 'diabetes']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True, withStd=True)\n",
        "\n",
        "# Define the SVM model\n",
        "svm = LinearSVC(featuresCol='scaled_features', labelCol='diabetes')\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline_svm = Pipeline(stages=[vector_assembler, scaler, svm])\n",
        "\n",
        "# Train the SVM model\n",
        "model_svm = pipeline_svm.fit(df_spark_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "df_spark_test_predictions_svm = model_svm.transform(df_spark_test)\n",
        "\n",
        "# Evaluate the model using MulticlassClassificationEvaluator\n",
        "evaluator_svm = MulticlassClassificationEvaluator(labelCol='diabetes', metricName='accuracy')\n",
        "accuracy_svm = evaluator_svm.evaluate(df_spark_test_predictions_svm)\n",
        "\n",
        "# Print SVM Test Accuracy\n",
        "print(\"SVM Test Accuracy:\", accuracy_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxSV0BCWEeqP",
        "outputId": "8587c5fc-61c2-4155-951b-f3c2501ba3b0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Test Accuracy: 0.8839852892009361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Model Precission, Recall, F1 Scores and Confusion Matrix\n"
      ],
      "metadata": {
        "id": "JeqvFz6dJdAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "feature_columns = [col_name for col_name in df_spark_train.columns if col_name != 'diabetes']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True, withStd=True)\n",
        "\n",
        "# Define the SVM model\n",
        "svm = LinearSVC(featuresCol='scaled_features', labelCol='diabetes')\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline_svm = Pipeline(stages=[vector_assembler, scaler, svm])\n",
        "\n",
        "# Train the SVM model\n",
        "model_svm = pipeline_svm.fit(df_spark_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "df_spark_test_predictions_svm = model_svm.transform(df_spark_test)\n",
        "\n",
        "# Evaluate the model using MulticlassClassificationEvaluator\n",
        "evaluator_svm = MulticlassClassificationEvaluator(labelCol='diabetes', metricName='weightedPrecision')\n",
        "precision_svm = evaluator_svm.evaluate(df_spark_test_predictions_svm)\n",
        "\n",
        "evaluator_svm = MulticlassClassificationEvaluator(labelCol='diabetes', metricName='weightedRecall')\n",
        "recall_svm = evaluator_svm.evaluate(df_spark_test_predictions_svm)\n",
        "\n",
        "evaluator_svm = MulticlassClassificationEvaluator(labelCol='diabetes', metricName='f1')\n",
        "f1_svm = evaluator_svm.evaluate(df_spark_test_predictions_svm)\n",
        "\n",
        "# Extract predictions and labels as RDD for confusion matrix\n",
        "prediction_and_label = df_spark_test_predictions_svm.select(\"prediction\", \"diabetes\").rdd\n",
        "\n",
        "# Instantiate MulticlassMetrics\n",
        "metrics_svm = MulticlassMetrics(prediction_and_label)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"SVM Precision:\", precision_svm)\n",
        "print(\"SVM Recall:\", recall_svm)\n",
        "print(\"SVM F1 Score:\", f1_svm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pAnuxZqFWX1",
        "outputId": "f89cc80a-1480-4aa0-b9e7-fd553a8fc550"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Precision: 0.8839668604721829\n",
            "SVM Recall: 0.8839852892009361\n",
            "SVM F1 Score: 0.8839402784519845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "# Create a temporary view for Spark SQL\n",
        "df_spark_test_predictions_svm.createOrReplaceTempView(\"predictions\")\n",
        "\n",
        "# Use Spark SQL to calculate confusion matrix\n",
        "confusion_matrix_svm = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        SUM(CASE WHEN prediction = 1 AND diabetes = 1 THEN 1 ELSE 0 END) AS true_positive,\n",
        "        SUM(CASE WHEN prediction = 0 AND diabetes = 1 THEN 1 ELSE 0 END) AS false_negative,\n",
        "        SUM(CASE WHEN prediction = 1 AND diabetes = 0 THEN 1 ELSE 0 END) AS false_positive,\n",
        "        SUM(CASE WHEN prediction = 0 AND diabetes = 0 THEN 1 ELSE 0 END) AS true_negative\n",
        "    FROM predictions\n",
        "\"\"\").collect()[0]\n",
        "\n",
        "# Convert the result to a Python dictionary\n",
        "confusion_matrix_svm_dict = confusion_matrix_svm.asDict()\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix for SVM:\")\n",
        "print(confusion_matrix_svm_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epq6QqHCF5Xm",
        "outputId": "81c1352b-7e17-4dd0-c064-d1f402824887"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for SVM:\n",
            "{'true_positive': 1231, 'false_negative': 183, 'false_positive': 164, 'true_negative': 1413}\n"
          ]
        }
      ]
    }
  ]
}